---
title: "Cleaning Data"
author: "Renato Erazo"
date: "14/12/2020"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## ===========================  01 Common Data Problems

```{r}
bike_share_rides <- readRDS("bike_share_rides_ch1_1.rds")
```

Exercise
Converting data types

Throughout this chapter, you'll be working with San Francisco bike share ride data called bike_share_rides. It contains information on start and end stations of each trip, the trip duration, and some user information.

Before beginning to analyze any dataset, it's important to take a look at the different types of columns you'll be working with, which you can do using glimpse().

In this exercise, you'll take a look at the data types contained in bike_share_rides and see how an incorrect data type can flaw your analysis.

dplyr and assertive are loaded and bike_share_rides is available.
Instructions 1/3
50 XP

    1
    2
    3

    Examine the data types of the columns of bike_share_rides.
    Get a summary of the user_birth_year column of bike_share_rides.
```{r}
library(dplyr)
install.packages("assertive")
library(assertive)

```
```{r}
library(dplyr)
library(assertive)

# Glimpse at bike_share_rides
glimpse(bike_share_rides)

# Summary of user_birth_year

summary(bike_share_rides$user_birth_year)
```

Exercise
Converting data types

Throughout this chapter, you'll be working with San Francisco bike share ride data called bike_share_rides. It contains information on start and end stations of each trip, the trip duration, and some user information.

Before beginning to analyze any dataset, it's important to take a look at the different types of columns you'll be working with, which you can do using glimpse().

In this exercise, you'll take a look at the data types contained in bike_share_rides and see how an incorrect data type can flaw your analysis.

dplyr and assertive are loaded and bike_share_rides is available.
Instructions 3/3
50 XP

    3

    Add a new column to bike_share_rides called user_birth_year_fct, which contains user_birth_year, converted to a factor.
    Assert that the user_birth_year_fct is a factor to confirm the conversion.

```{r}
# Glimpse at bike_share_rides
glimpse(bike_share_rides)

# Summary of user_birth_year
summary(bike_share_rides$user_birth_year)

# Convert user_birth_year to factor: user_birth_year_fct
bike_share_rides <- bike_share_rides %>%
  mutate(user_birth_year_fct = as.factor(bike_share_rides$user_birth_year))

# Assert user_birth_year_fct is a factor
assert_is_factor(bike_share_rides$user_birth_year_fct)

# Summary of user_birth_year_fct
summary(bike_share_rides$user_birth_year_fct)
```

Exercise
Trimming strings

In the previous exercise, you were able to identify the correct data type and convert user_birth_year to the correct type, allowing you to extract counts that gave you a bit more insight into the dataset.

Another common dirty data problem is having extra bits like percent signs or periods in numbers, causing them to be read in as characters. In order to be able to crunch these numbers, the extra bits need to be removed and the numbers need to be converted from character to numeric. In this exercise, you'll need to convert the duration column from character to numeric, but before this can happen, the word "minutes" needs to be removed from each value.

dplyr, assertive, and stringr are loaded and bike_share_rides is available.
Instructions
100 XP

    Use str_remove() to remove "minutes" from the duration column of bike_share_rides. Add this as a new column called duration_trimmed.
    Convert the duration_trimmed column to a numeric type and add this as a new column called duration_mins.
    Glimpse at bike_share_rides and assert that the duration_mins column is numeric.
    Calculate the mean of duration_mins.

```{r}
library(dplyr)
library(stringr)
library(assertive)
bike_share_rides <- bike_share_rides %>%
  # Remove 'minutes' from duration: duration_trimmed
  mutate(duration_trimmed = str_remove(duration,"minutes"),
         # Convert duration_trimmed to numeric: duration_mins
         duration_min = as.numeric(duration_trimmed))

# Glimpse at bike_share_rides
glimpse(bike_share_rides)

# Assert duration_mins is numeric
assert_is_numeric(bike_share_rides$duration_mins)

# Calculate mean duration
mean(bike_share_rides$duration_mins)
```

Exercise
Ride duration constraints

Values that are out of range can throw off an analysis, so it's important to catch them early on. In this exercise, you'll be examining the duration_min column more closely. Bikes are not allowed to be kept out for more than 24 hours, or 1440 minutes at a time, but issues with some of the bikes caused inaccurate recording of the time they were returned.

In this exercise, you'll replace erroneous data with the range limit (1440 minutes), however, you could just as easily replace these values with NAs.

dplyr, assertive, and ggplot2 are loaded and bike_share_rides is available.
Instructions 1/2
50 XP

    1
    2

    Create a three-bin histogram of the duration_min column of bike_share_rides using ggplot2 to identify if there is out-of-range data.


```{r}
library(ggplot2)
# Create breaks
breaks <- c(min(bike_share_rides$duration_mins), 0, 1440, max(bike_share_rides$duration_mins))

# Create a histogram of duration_min
ggplot(bike_share_rides, aes(duration_mins)) +
  geom_histogram(breaks = breaks)
```

Exercise
Ride duration constraints

Values that are out of range can throw off an analysis, so it's important to catch them early on. In this exercise, you'll be examining the duration_min column more closely. Bikes are not allowed to be kept out for more than 24 hours, or 1440 minutes at a time, but issues with some of the bikes caused inaccurate recording of the time they were returned.

In this exercise, you'll replace erroneous data with the range limit (1440 minutes), however, you could just as easily replace these values with NAs.

dplyr, assertive, and ggplot2 are loaded and bike_share_rides is available.
Instructions 2/2
50 XP

    2

    Replace the values of duration_min that are greater than 1440 minutes (24 hours) with 1440. Add this to bike_share_rides as a new column called duration_min_const.
    Assert that all values of duration_min_const are between 0 and 1440.

```{r}
library(dplyr)
library(assertive)
library(ggplot2)
# Create breaks
breaks <- c(min(bike_share_rides$duration_min), 0, 1440, max(bike_share_rides$duration_min))

# Create a histogram of duration_min
ggplot(bike_share_rides, aes(duration_min)) +
  geom_histogram(breaks = breaks)

# duration_min_const: replace vals of duration_min > 1440 with 1440
bike_share_rides <- bike_share_rides %>%
  mutate(duration_min_const = replace(duration_min, duration_min > 1440, 1440))

# Make sure all values of duration_min_const are between 0 and 1440
assert_all_are_in_closed_range(bike_share_rides$duration_min_const, lower = 0, upper = 1440)
```


Exercise
Back to the future

Something has gone wrong and it looks like you have data with dates from the future, which is way outside of the date range you expected to be working with. To fix this, you'll need to remove any rides from the dataset that have a date in the future. Before you can do this, the date column needs to be converted from a character to a Date. Having these as Date objects will make it much easier to figure out which rides are from the future, since R makes it easy to check if one Date object is before (<) or after (>) another.

dplyr and assertive are loaded and bike_share_rides is available.
Instructions
100 XP

    Convert the date column of bike_share_rides from character to the Date data type.
    Assert that all values in the date column happened sometime in the past and not in the future.
    Filter bike_share_rides to get only the rides from the past or today, and save this as bike_share_rides_past.
    Assert that the dates in bike_share_rides_past occurred only in the past.


```{r}
library(lubridate)
# Convert date to Date type
bike_share_rides <- bike_share_rides %>%
  mutate(date = as.Date(date))

# Make sure all dates are in the past
assert_all_are_in_past(bike_share_rides$date)


# Filter for rides that occurred before or on today's date
bike_share_rides_past <- bike_share_rides %>%
  filter(date <= today())

# Make sure all dates from bike_share_rides_past are in the past
assert_all_are_in_past(bike_share_rides_past$date)
```

Exercise
Full duplicates

You've been notified that an update has been made to the bike sharing data pipeline to make it more efficient, but that duplicates are more likely to be generated as a result. To make sure that you can continue using the same scripts to run your weekly analyses about ride statistics, you'll need to ensure that any duplicates in the dataset are removed first.

When multiple rows of a data frame share the same values for all columns, they're full duplicates of each other. Removing duplicates like this is important, since having the same value repeated multiple times can alter summary statistics like the mean and median. Each ride, including its ride_id should be unique.

dplyr is loaded and bike_share_rides is available.
Instructions
100 XP

    Get the total number of full duplicates in bike_share_rides.
    Remove all full duplicates from bike_share_rides and save the new data frame as bike_share_rides_unique.
    Get the total number of full duplicates in the new bike_share_rides_unique data frame.

```{r}
# Count the number of full duplicates
sum(duplicated(bike_share_rides))

# Remove duplicates
bike_share_rides_unique <- distinct(bike_share_rides)

# Count the full duplicates in bike_share_rides_unique
sum(duplicated(bike_share_rides_unique))
```
Exercise
Removing partial duplicates

Now that you've identified and removed the full duplicates, it's time to check for partial duplicates. Partial duplicates are a bit tricker to deal with than full duplicates. In this exercise, you'll first identify any partial duplicates and then practice the most common technique to deal with them, which involves dropping all partial duplicates, keeping only the first.

dplyr is loaded and bike_share_rides is available.
Instructions 1/3
35 XP

    1
    2
    3

    Count the number of occurrences of each ride_id.
    Filter for ride_ids that occur multiple times.
```{r}
# Find duplicated ride_ids
bike_share_rides %>% 
  # Count the number of occurrences of each ride_id
  count(ride_id) %>% 
  # Filter for rows with a count > 1
  filter(n > 1)
```
Exercise
Removing partial duplicates

Now that you've identified and removed the full duplicates, it's time to check for partial duplicates. Partial duplicates are a bit tricker to deal with than full duplicates. In this exercise, you'll first identify any partial duplicates and then practice the most common technique to deal with them, which involves dropping all partial duplicates, keeping only the first.

dplyr is loaded and bike_share_rides is available.
Instructions 2/3
35 XP

    2
    3

    Remove full and partial duplicates from bike_share_rides based on ride_id only, keeping all columns.
    Store this as bike_share_rides_unique.

```{r}
# Find duplicated ride_ids
bike_share_rides %>% 
  count(ride_id) %>% 
  filter(n > 1)

# Remove full and partial duplicates
bike_share_rides_unique <- bike_share_rides %>%
  # Only based on ride_id instead of all cols
   distinct(ride_id,.keep_all = TRUE)
```

Removing partial duplicates

Now that you've identified and removed the full duplicates, it's time to check for partial duplicates. Partial duplicates are a bit tricker to deal with than full duplicates. In this exercise, you'll first identify any partial duplicates and then practice the most common technique to deal with them, which involves dropping all partial duplicates, keeping only the first.

dplyr is loaded and bike_share_rides is available.


```{r}
# Find duplicated ride_ids
bike_share_rides %>% 
  count(ride_id) %>% 
  filter(n > 1)

# Remove full and partial duplicates
bike_share_rides_unique <- bike_share_rides %>%
  # Only based on ride_id instead of all cols
  distinct(ride_id, .keep_all = TRUE)

# Find duplicated ride_ids in bike_share_rides_unique
bike_share_rides_unique %>%
  # Count the number of occurrences of each ride_id
  count(ride_id) %>%
  # Filter for rows with a count > 1
  filter(n>1)
```
Exercise
Aggregating partial duplicates

Another way of handling partial duplicates is to compute a summary statistic of the values that differ between partial duplicates, such as mean, median, maximum, or minimum. This can come in handy when you're not sure how your data was collected and want an average, or if based on domain knowledge, you'd rather have too high of an estimate than too low of an estimate (or vice versa).

dplyr is loaded and bike_share_rides is available.
Instructions
100 XP

    Group bike_share_rides by ride_id and date.
    Add a column called duration_min_avg that contains the mean ride duration for the row's ride_id and date.
    Remove duplicates based on ride_id and date, keeping all columns of the data frame.
    Remove the duration_min column.

```{r}
bike_share_rides %>%
  # Group by ride_id and date
  group_by(ride_id,date) %>%
  # Add duration_min_avg column
  mutate(duration_min_avg = mean(duration_min) ) %>%
  # Remove duplicates based on ride_id and date, keep all cols
  distinct(ride_id,date,.keep_all=TRUE) %>%
  # Remove duration_min column
  select(-duration_min)
```

## ========================== Categorical and text data




```{r}
sfo_survey <- readRDS("sfo_survey_ch2_1.rds")
library(dplyr)
sfo_survey %>% count(dest_size)
sfo_survey %>% count(cleanliness)
```

Exercise
Correcting inconsistency

Now that you've identified that dest_size has whitespace inconsistencies and cleanliness has capitalization inconsistencies, you'll use the new tools at your disposal to fix the inconsistent values in sfo_survey instead of removing the data points entirely, which could add bias to your dataset if more than 5% of the data points need to be dropped.

dplyr and stringr are loaded and sfo_survey is available.
Instructions
100 XP

    Add a column to sfo_survey called dest_size_trimmed that contains the values in the dest_size column with all leading and trailing whitespace removed.
    Add another column called cleanliness_lower that contains the values in the cleanliness column converted to all lowercase.
    Count the number of occurrences of each category in dest_size_trimmed.
    Count the number of occurrences of each category in cleanliness_lower.

```{r}
library(dplyr)
library(stringr)
# Add new columns to sfo_survey
sfo_survey <- sfo_survey %>%
  # dest_size_trimmed: dest_size without whitespace
  mutate(dest_size_trimmed = str_trim(dest_size),
         # cleanliness_lower: cleanliness converted to lowercase
         cleanliness_lower = str_to_lower(cleanliness))

# Count values of dest_size_trimmed
sfo_survey %>%
  count(dest_size_trimmed)

# Count values of cleanliness_lower
sfo_survey %>%
  count(cleanliness_lower)
```

Exercise
Collapsing categories

One of the tablets that participants filled out the sfo_survey on was not properly configured, allowing the response for dest_region to be free text instead of a dropdown menu. This resulted in some inconsistencies in the dest_region variable that you'll need to correct in this exercise to ensure that the numbers you report to your boss are as accurate as possible.

dplyr and forcats are loaded and sfo_survey is available.
Instructions 1/3
0 XP

    1
    2
    3

    Count the categories of dest_region.


```{r}
# Count categories of dest_region
sfo_survey %>%
  count(dest_region)
```

Exercise
Collapsing categories

One of the tablets that participants filled out the sfo_survey on was not properly configured, allowing the response for dest_region to be free text instead of a dropdown menu. This resulted in some inconsistencies in the dest_region variable that you'll need to correct in this exercise to ensure that the numbers you report to your boss are as accurate as possible.

dplyr and forcats are loaded and sfo_survey is available.
Instructions 3/3
100 XP

    3

    Create a vector called europe_categories containing the three values of dest_region that need to be collapsed.
    Add a new column to sfo_survey called dest_region_collapsed that contains the values from the dest_region column, except the categories stored in europe_categories should be collapsed to Europe.
    Count the categories of dest_region_collapsed

```{r}
library(dplyr)
library(forcats)
# Count categories of dest_region
sfo_survey %>%
  count(dest_region)

# Categories to map to Europe
europe_categories <- c("EU", "eur", "Europ")

# Add a new col dest_region_collapsed
sfo_survey %>%
  # Map all categories in europe_categories to Europe
  mutate(dest_region_collapsed = fct_collapse(dest_region, Europe = europe_categories)) %>%
  # Count categories of dest_region_collapsed
  count(dest_region_collapsed)
```
## ==== Text data inconsistences

Exercise
Detecting inconsistent text data

You've recently received some news that the customer support team wants to ask the SFO survey participants some follow-up questions. However, the auto-dialer that the call center uses isn't able to parse all of the phone numbers since they're all in different formats. After some investigation, you found that some phone numbers are written with hyphens (-) and some are written with parentheses ((,)). In this exercise, you'll figure out which phone numbers have these issues so that you know which ones need fixing.

dplyr and stringr are loaded, and sfo_survey is available.
Instructions 1/2
50 XP

    1
        Filter for rows with phone numbers that contain "-"s.
```{r}
library(stringr)
library(dplyr)
# Filter for rows with "-" in the phone column
sfo_survey %>%
  filter(str_detect(phone,"-"))
```

## ================ Completeness

Librería para detectar archivos con valores perdidos (missing)



```{r}
install.packages("visdat")
```

Verificar los na's 


```{r}
library(visdat)
sum(is.na(airquality))
vis_miss(airquality)
```

## ============================= 04 Record Linkage

String distance

```{r}
install.packages("stringdist")
```

Distancia entre puffin y muffins

```{r}
library(stringdist)
stringdist("puffin","muffins",method = "dl")
```

Exercise
Small distance, small difference

In the video exercise, you learned that there are multiple ways to calculate how similar or different two strings are. Now you'll practice using the stringdist package to compute string distances using various methods. It's important to be familiar with different methods, as some methods work better on certain datasets, while others work better on other datasets.

The stringdist package has been loaded for you.
Instructions 1/4
25 XP

    1
    2
    3
    4

    Calculate the Damerau-Levenshtein distance between "las angelos" and "los angeles".


```{r}
# Calculate Damerau-Levenshtein distance
stringdist("Las angelos", "Los angeles", method = "dl")
```

Exercise
Small distance, small difference

In the video exercise, you learned that there are multiple ways to calculate how similar or different two strings are. Now you'll practice using the stringdist package to compute string distances using various methods. It's important to be familiar with different methods, as some methods work better on certain datasets, while others work better on other datasets.

The stringdist package has been loaded for you.
Instructions 2/4
25 XP

    2
    3
    4

    Calculate the Longest Common Substring (LCS) distance between "las angelos" and "los angeles".


```{r}
# Calculate LCS distance
stringdist("Las angelos", "Los angeles", method = "lcs")
```

Exercise
Small distance, small difference

In the video exercise, you learned that there are multiple ways to calculate how similar or different two strings are. Now you'll practice using the stringdist package to compute string distances using various methods. It's important to be familiar with different methods, as some methods work better on certain datasets, while others work better on other datasets.

The stringdist package has been loaded for you.
Instructions 3/4
25 XP

    3
    4

    Calculate the Jaccard distance between "las angelos" and "los angeles".
```{r}
# Calculate Jaccard distance
stringdist("Las angelos", "Los angeles", method = "jaccard")
```

Leer zagat dataset

```{r}
zagat <- readRDS("zagat.rds")

##crear dataframe cities

cities <- data.frame(city_actual = c("new york","los angeles","atlanta","san francisco","las vegas"))

```
```{r}
library(dplyr)
# Count the number of each city variation

zagat %>%
  count(city)
```

Exercise
Fixing typos with string distance

In this chapter, one of the datasets you'll be working with, zagat, is a set of restaurants in New York, Los Angeles, Atlanta, San Francisco, and Las Vegas. The data is from Zagat, a company that collects restaurant reviews, and includes the restaurant names, addresses, phone numbers, as well as other restaurant information.

The city column contains the name of the city that the restaurant is located in. However, there are a number of typos throughout the column. Your task is to map each city to one of the five correctly-spelled cities contained in the cities data frame.

dplyr and fuzzyjoin are loaded, and zagat and cities are available.
Instructions 2/2
0 XP

    2

    Left join zagat and cities based on string distance using the city and city_actual columns.
    Select the name, city, and city_actual columns.

```{r}
install.packages("fuzzyjoin")
```

```{r}
library(fuzzyjoin)
# Count the number of each city variation
zagat %>%
  count(city)

# Join and look at results
zagat %>%
  # Left join based on stringdist using city and city_actual cols
  stringdist_left_join(cities, by = c("city" = "city_actual")) %>%
  # Select the name, city, and city_actual cols
  select(name, city, city_actual)
```

# A tibble: 310 x 3
   name                      city         city_actual
   <chr>                     <chr>        <fct>      
 1 apple pan the             llos angeles los angeles
 2 asahi ramen               los angeles  los angeles
 3 baja fresh                los angeles  los angeles
 4 belvedere the             los angeles  los angeles
 5 benita's frites           lo angeles   los angeles
 6 bernard's                 los angeles  los angeles
 7 bistro 45                 lo angeles   los angeles
 8 brighton coffee shop      los angeles  los angeles
 9 bristol farms market cafe los anegeles los angeles
10 cafe'50s                  los angeles  los angeles


Exercise
Pair blocking

Zagat and Fodor's are both companies that gather restaurant reviews. The zagat and fodors datasets both contain information about various restaurants, including addresses, phone numbers, and cuisine types. Some restaurants appear in both datasets, but don't necessarily have the same exact name or phone number written down. In this chapter, you'll work towards figuring out which restaurants appear in both datasets.

The first step towards this goal is to generate pairs of records so that you can compare them. In this exercise, you'll first generate all possible pairs, and then use your newly-cleaned city column as a blocking variable.

zagat and fodors are available.
Instructions 2/2
50 XP

        Load the reclin package.
        Generate all possible pairs of records between the zagat and fodors datasets.
    2
        Use pair blocking to generate only pairs that have matching values in the city column.

```{r}
install.packages("reclin")
```
        
        

```{r}
fodors <- readRDS("fodors.rds")

# Load reclin
library(reclin)

# Generate pairs with same city
pair_blocking(zagat,fodors,blocking_var = "city")
```

Exercise
Comparing pairs

Now that you've generated the pairs of restaurants, it's time to compare them. You can easily customize how you perform your comparisons using the by and default_comparator arguments. There's no right answer as to what each should be set to, so in this exercise, you'll try a couple options out.

dplyr and reclin are loaded and zagat and fodors are available.
Instructions 1/2
50 XP

    1
        Compare pairs by name using lcs() distance.

```{r}
# Generate pairs
pair_blocking(zagat, fodors, blocking_var = "city") %>%
  # Compare pairs by name using lcs()
  compare_pairs(by = "name",
      default_comparator = lcs())
```

Exercise
Comparing pairs

Now that you've generated the pairs of restaurants, it's time to compare them. You can easily customize how you perform your comparisons using the by and default_comparator arguments. There's no right answer as to what each should be set to, so in this exercise, you'll try a couple options out.

dplyr and reclin are loaded and zagat and fodors are available.
Instructions 2/2
50 XP

        Compare pairs by name using lcs() distance.
    2
        Compare pairs by name, phone, and addr using jaro_winkler().


```{r}
# Generate pairs
pair_blocking(zagat, fodors, blocking_var = "city") %>%
  # Compare pairs by name, phone, addr
  compare_pairs(by = c("name","phone","addr"),default_comparator = jaro_winkler())
```

Exercise
Putting it together

During this chapter, you've cleaned up the city column of zagat using string similarity, as well as generated and compared pairs of restaurants from zagat and fodors. The end is near - all that's left to do is score and select pairs and link the data together, and you'll be able to begin your analysis in no time!

reclin and dplyr are loaded and zagat and fodors are available.
Instructions 1/3
35 XP

    1
    2
    3

    Score the pairs of records probabilistically.


```{r}
# Create pairs
pair_blocking(zagat, fodors, blocking_var = "city") %>%
  # Compare pairs
  compare_pairs(by = "name", default_comparator = jaro_winkler()) %>%
  # Score pairs
  score_problink()
```

Exercise
Putting it together

During this chapter, you've cleaned up the city column of zagat using string similarity, as well as generated and compared pairs of restaurants from zagat and fodors. The end is near - all that's left to do is score and select pairs and link the data together, and you'll be able to begin your analysis in no time!

reclin and dplyr are loaded and zagat and fodors are available.
Instructions 2/3
35 XP

    2
    3

    Select the pairs that are considered matches.

```{r}
# Create pairs
pair_blocking(zagat, fodors, blocking_var = "city") %>%
  # Compare pairs
  compare_pairs(by = "name", default_comparator = jaro_winkler()) %>%
  # Score pairs
  score_problink() %>%
  # Select pairs
  select_n_to_m()
```

Exercise
Putting it together

During this chapter, you've cleaned up the city column of zagat using string similarity, as well as generated and compared pairs of restaurants from zagat and fodors. The end is near - all that's left to do is score and select pairs and link the data together, and you'll be able to begin your analysis in no time!

reclin and dplyr are loaded and zagat and fodors are available.
Instructions 3/3
30 XP

    3

    Link the two data frames together.
```{r}
# Create pairs
pair_blocking(zagat, fodors, blocking_var = "city") %>%
  # Compare pairs
  compare_pairs(by = "name", default_comparator = jaro_winkler()) %>%
  # Score pairs
  score_problink() %>%
  # Select pairs
  select_n_to_m() %>%
  # Link data 
  link()
```




